{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, UpSampling2D, Conv2D, MaxPooling2D, Activation\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import params\n",
    "import dataset\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow\n",
    "import gc\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "    \n",
    "    tensorflow.reset_default_graph()\n",
    "\n",
    "    try:\n",
    "        del classifier # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "# reset_keras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def BuildModel_SegNet():\n",
    "    inp = Input(shape=(params.GetImageSize(), params.GetImageSize(), params.GetChannelsNum()))\n",
    "\n",
    "    conv_1_1 = Conv2D(32, (3, 3), padding='same')(inp)\n",
    "    conv_1_1 = Activation('relu')(conv_1_1)\n",
    "    conv_1_2 = Conv2D(32, (3, 3), padding='same')(conv_1_1)\n",
    "    conv_1_2 = Activation('relu')(conv_1_2)\n",
    "    pool_1 = MaxPooling2D(2)(conv_1_2)\n",
    "\n",
    "    conv_2_1 = Conv2D(64, (3, 3), padding='same')(pool_1)\n",
    "    conv_2_1 = Activation('relu')(conv_2_1)\n",
    "    conv_2_2 = Conv2D(64, (3, 3), padding='same')(conv_2_1)\n",
    "    conv_2_2 = Activation('relu')(conv_2_2)\n",
    "    pool_2 = MaxPooling2D(2)(conv_2_2)\n",
    "\n",
    "    conv_3_1 = Conv2D(128, (3, 3), padding='same')(pool_2)\n",
    "    conv_3_1 = Activation('relu')(conv_3_1)\n",
    "    conv_3_2 = Conv2D(128, (3, 3), padding='same')(conv_3_1)\n",
    "    conv_3_2 = Activation('relu')(conv_3_2)\n",
    "    pool_3 = MaxPooling2D(2)(conv_3_2)\n",
    "\n",
    "    up_1 = UpSampling2D(2, interpolation='bilinear')(pool_3)\n",
    "    conv_up_1_1 = Conv2D(256, (3, 3), padding='same')(up_1)\n",
    "    conv_up_1_1 = Activation('relu')(conv_up_1_1)\n",
    "    conv_up_1_2 = Conv2D(256, (3, 3), padding='same')(conv_up_1_1)\n",
    "    conv_up_1_2 = Activation('relu')(conv_up_1_2)\n",
    "\n",
    "    up_2 = UpSampling2D(2, interpolation='bilinear')(conv_up_1_2)\n",
    "    conv_up_2_1 = Conv2D(128, (3, 3), padding='same')(up_2)\n",
    "    conv_up_2_1 = Activation('relu')(conv_up_2_1)\n",
    "    conv_up_2_2 = Conv2D(128, (3, 3), padding='same')(conv_up_2_1)\n",
    "    conv_up_2_2 = Activation('relu')(conv_up_2_2)\n",
    "\n",
    "    up_3 = UpSampling2D(2, interpolation='bilinear')(conv_up_2_2)\n",
    "    conv_up_3_1 = Conv2D(64, (3, 3), padding='same')(up_3)\n",
    "    conv_up_3_1 = Activation('relu')(conv_up_3_1)\n",
    "    conv_up_3_2 = Conv2D(4, (3, 3), padding='same')(conv_up_3_1)\n",
    "    result = Activation('sigmoid')(conv_up_3_2)\n",
    "    \n",
    "    return Model(inputs=inp, outputs=result)\n",
    "\n",
    "best_w = keras.callbacks.ModelCheckpoint('segnet_best.h5',\n",
    "                                monitor='val_loss',\n",
    "                                verbose=0,\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=False,\n",
    "                                mode='auto',\n",
    "                                period=1)\n",
    "\n",
    "last_w = keras.callbacks.ModelCheckpoint('segnet_last.h5',\n",
    "                                monitor='val_loss',\n",
    "                                verbose=0,\n",
    "                                save_best_only=False,\n",
    "                                save_weights_only=False,\n",
    "                                mode='auto',\n",
    "                                period=1)\n",
    "\n",
    "\n",
    "callbacks = [best_w, last_w]\n",
    "\n",
    "\n",
    "model = BuildModel_SegNet()\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(adam, 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildModel_UNet():\n",
    "    inp = Input(shape=(params.GetImageSize(), params.GetImageSize(), params.GetChannelsNum()))\n",
    "\n",
    "    conv_1_1 = Conv2D(32, (3, 3), padding='same')(inp)\n",
    "    conv_1_1 = Activation('relu')(conv_1_1)\n",
    "    conv_1_2 = Conv2D(32, (3, 3), padding='same')(conv_1_1)\n",
    "    conv_1_2 = Activation('relu')(conv_1_2)\n",
    "    pool_1 = MaxPooling2D(2)(conv_1_2)\n",
    "\n",
    "    conv_2_1 = Conv2D(64, (3, 3), padding='same')(pool_1)\n",
    "    conv_2_1 = Activation('relu')(conv_2_1)\n",
    "    conv_2_2 = Conv2D(64, (3, 3), padding='same')(conv_2_1)\n",
    "    conv_2_2 = Activation('relu')(conv_2_2)\n",
    "    pool_2 = MaxPooling2D(2)(conv_2_2)\n",
    "\n",
    "    conv_3_1 = Conv2D(128, (3, 3), padding='same')(pool_2)\n",
    "    conv_3_1 = Activation('relu')(conv_3_1)\n",
    "    conv_3_2 = Conv2D(128, (3, 3), padding='same')(conv_3_1)\n",
    "    conv_3_2 = Activation('relu')(conv_3_2)\n",
    "    pool_3 = MaxPooling2D(2)(conv_3_2)\n",
    "\n",
    "    up_1 = UpSampling2D(2, interpolation='bilinear')(pool_3)\n",
    "    conc_1 = Concatenate()([conv_3_2, up_1])\n",
    "    conv_up_1_1 = Conv2D(256, (3, 3), padding='same')(conc_1)\n",
    "    conv_up_1_1 = Activation('relu')(conv_up_1_1)\n",
    "    conv_up_1_2 = Conv2D(256, (3, 3), padding='same')(conv_up_1_1)\n",
    "    conv_up_1_2 = Activation('relu')(conv_up_1_2)\n",
    "\n",
    "    up_2 = UpSampling2D(2, interpolation='bilinear')(conv_up_1_2)\n",
    "    conc_2 = Concatenate()([conv_2_2, up_2])\n",
    "    conv_up_2_1 = Conv2D(128, (3, 3), padding='same')(conc_2)\n",
    "    conv_up_2_1 = Activation('relu')(conv_up_2_1)\n",
    "    conv_up_2_2 = Conv2D(128, (3, 3), padding='same')(conv_up_2_1)\n",
    "    conv_up_2_2 = Activation('relu')(conv_up_2_2)\n",
    "\n",
    "    up_3 = UpSampling2D(2, interpolation='bilinear')(conv_up_2_2)\n",
    "    conc_3 = Concatenate()([conv_1_2, up_3])\n",
    "    conv_up_3_1 = Conv2D(64, (3, 3), padding='same')(conc_3)\n",
    "    conv_up_3_1 = Activation('relu')(conv_up_3_1)\n",
    "    conv_up_3_2 = Conv2D(4, (3, 3), padding='same')(conv_up_3_1)\n",
    "    result = Activation('sigmoid')(conv_up_3_2)\n",
    "    \n",
    "    return Model(inputs=inp, outputs=result)\n",
    "\n",
    "net_name = 'unet'\n",
    "best_w_file = net_name + '_best.h5'\n",
    "last_w_file = net_name + '_last.h5'\n",
    "\n",
    "best_w = keras.callbacks.ModelCheckpoint(best_w_file,\n",
    "                                monitor='val_loss',\n",
    "                                verbose=0,\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=False,\n",
    "                                mode='auto',\n",
    "                                period=1)\n",
    "\n",
    "last_w = keras.callbacks.ModelCheckpoint(last_w_file,\n",
    "                                monitor='val_loss',\n",
    "                                verbose=0,\n",
    "                                save_best_only=False,\n",
    "                                save_weights_only=False,\n",
    "                                mode='auto',\n",
    "                                period=1)\n",
    "\n",
    "\n",
    "callbacks = [best_w, last_w]\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "weights = np.array([11.333580017089844, 80.10086822509766, 8.1307373046875, 0.434814453125]).astype(np.float32) / 100.0\n",
    "weights = (np.sum(weights) - weights / np.sum(weights))\n",
    "# print(weights)\n",
    "\n",
    "def CreateModel():\n",
    "    model = BuildModel_UNet()\n",
    "    model.compile(\n",
    "        optimizer=adam,\n",
    "        metrics=['accuracy'], \n",
    "        loss=weighted_categorical_crossentropy(weights)\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete reading input data. Will Now print a snippet of it\n",
      "Number of files in Training-set:\t\t10000\n",
      "Number of files in Validation-set:\t2000\n"
     ]
    }
   ],
   "source": [
    "data = dataset.read_train_validation_big_sets(\n",
    "        train_path = os.path.join(params.training_data_path),\n",
    "        validation_path = os.path.join(params.validation_data_path),\n",
    "        image_size = params.GetImageSize())\n",
    "\n",
    "print(\"Complete reading input data. Will Now print a snippet of it\")\n",
    "print(\"Number of files in Training-set:\\t\\t{}\".format(data.train.num_examples))\n",
    "print(\"Number of files in Validation-set:\\t{}\".format(data.valid.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img_df = data.train.image_paths\n",
    "mask_df = data.train.masks_paths\n",
    "batch_size = 26\n",
    "\n",
    "index = 0\n",
    "while index < 5:\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    r_batch = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img_name = img_df[index]\n",
    "        mask_name = mask_df[index]\n",
    "\n",
    "#             print(index, img_name, mask_name)\n",
    "\n",
    "        index = (index + 1) % len(img_df)\n",
    "\n",
    "        img = cv2.imread(img_name, cv2.IMREAD_COLOR)\n",
    "        mask = cv2.imread(mask_name, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#             img, mask\n",
    "\n",
    "        img = cv2.resize(img, (params.GetImageSize(), params.GetImageSize()))\n",
    "        mask = cv2.resize(mask, (params.GetImageSize(), params.GetImageSize()))            \n",
    "\n",
    "        height, width = mask.shape\n",
    "        masks = np.zeros((height, width, 4))\n",
    "        for i in range(masks.shape[2]):\n",
    "            masks[:, :, i] = (mask == (i+1))\n",
    "\n",
    "        #moving the channel:\n",
    "        mask_train = np.moveaxis(masks,-1,1)\n",
    "\n",
    "        x_batch += [img]\n",
    "        y_batch.append(mask_train)\n",
    "        r_batch += [mask]\n",
    "\n",
    "    x_batch = np.array(x_batch) / 255.\n",
    "    y_batch = np.array(y_batch) / 255.\n",
    "    r_batch = np.array(r_batch) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_batch.shape,y_batch.shape,r_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_with_mask(img_name, mask_name):\n",
    "    img = cv2.imread(img_name, cv2.IMREAD_COLOR)\n",
    "    mask = cv2.imread(mask_name, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "#     img, mask\n",
    "            \n",
    "    img = cv2.resize(img, (params.GetImageSize(), params.GetImageSize()))\n",
    "    mask = cv2.resize(mask, (params.GetImageSize(), params.GetImageSize()))            \n",
    "\n",
    "    height, width = mask.shape\n",
    "    masks = np.zeros((height, width, 4))\n",
    "    for i in range(masks.shape[2]):\n",
    "        masks[:, :, i] = (mask == (i + 1))\n",
    "            \n",
    "#     masks = np.moveaxis(masks,-1,1)\n",
    "\n",
    "    return img / 255., masks / 1.\n",
    "\n",
    "def prepare_image_and_mask_for_prediction(img_name, mask_name):\n",
    "    img, masks = read_image_with_mask(img_name, mask_name)\n",
    "    return np.array([img]), np.array([masks])\n",
    "\n",
    "def keras_generator(img_df, mask_df, batch_size, iteration):\n",
    "    index = iteration * batch_size\n",
    "    print('keras_generator.index = {}'.format(index))\n",
    "    while True:\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            img_name = img_df[index]\n",
    "            mask_name = mask_df[index]\n",
    "            \n",
    "#             print(index, img_name, mask_name)\n",
    "            \n",
    "            index = (index + 1) % len(img_df)\n",
    "            \n",
    "            img, masks = read_image_with_mask(img_name, mask_name)\n",
    "            \n",
    "            x_batch += [img]\n",
    "            y_batch.append(masks)\n",
    "\n",
    "        \n",
    "#         print(x_batch.shape,y_batch.shape)\n",
    "\n",
    "        yield np.array(x_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_keras_history_to_all_history(common_history, keras_history):\n",
    "    if len(common_history) == 0:\n",
    "        common_history = {\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'loss': [],\n",
    "            'acc': []\n",
    "        }\n",
    "    \n",
    "    for key in keras_history.history.keys():\n",
    "        for value in keras_history.history[key]:\n",
    "            common_history[key].append(value)\n",
    "    \n",
    "    return common_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0...\n",
      "Epoch 1/1\n",
      "keras_generator.index = 0\n",
      "keras_generator.index = 0\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "full_history = {}\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "iterations = 2\n",
    "epochs_per_iteration = 1\n",
    "\n",
    "model_stage_file = 'model_weights_stage.h5'\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    print('Iteration {}...'.format(iteration))\n",
    "    \n",
    "    model = CreateModel()\n",
    "    \n",
    "    if (os.path.exists(model_stage_file)):\n",
    "        model = load_model(model_stage_file, custom_objects = {'loss': weighted_categorical_crossentropy(weights)})\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "                keras_generator(data.train.image_paths, data.train.masks_paths, batch_size, iteration),\n",
    "                steps_per_epoch = 30,\n",
    "                epochs = epochs_per_iteration,\n",
    "                verbose = 1,\n",
    "                callbacks = callbacks,\n",
    "                validation_data = keras_generator(data.valid.image_paths, data.valid.masks_paths, batch_size, iteration),\n",
    "                validation_steps = 60,\n",
    "                class_weight = None,\n",
    "                max_queue_size = 10,\n",
    "                workers = 1,\n",
    "                use_multiprocessing = False,\n",
    "                shuffle = True,\n",
    "                initial_epoch = 0\n",
    "    )\n",
    "    \n",
    "    print(history.history)\n",
    "    \n",
    "    model.save(model_stage_file)\n",
    "\n",
    "    full_history = append_keras_history_to_all_history(full_history, history)\n",
    "\n",
    "    del history\n",
    "    del model\n",
    "#     gc.collect()\n",
    "    reset_keras()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list all data in history\n",
    "# print(history.history.keys())\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# list all data in history\n",
    "print(full_history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(full_history['acc'])\n",
    "plt.plot(full_history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(all_history['loss'])\n",
    "plt.plot(all_history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model1 = BuildModel_SegNet()\n",
    "model1.load_weights('unet_best.h5')\n",
    "# model1 = load_model('unet_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# model1 = load_model('segnet_best.h5')\n",
    "model1 = load_model(best_w_file, custom_objects={'loss': weighted_categorical_crossentropy(weights)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in keras_generator(data.valid.image_paths, data.valid.masks_paths, 16, 0):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model1.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred[:,:,:,2] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassesMap(pred):\n",
    "    res = np.zeros((pred.shape[0], pred.shape[1]), np.uint8)\n",
    "    for r in range(pred.shape[0]):\n",
    "        for c in range(pred.shape[1]):\n",
    "            pred_class_index = np.argmax(pred[r,c,:])\n",
    "            res[r,c] = pred_class_index\n",
    "    res.shape\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 11\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(25, 25))\n",
    "axes[0][0].imshow(x[sample_index])\n",
    "im_0_1 = axes[0][1].imshow(pred[sample_index, ..., 0])\n",
    "im_1_0 = axes[1][0].imshow(pred[sample_index, ..., 1])\n",
    "im_1_1 = axes[1][1].imshow(pred[sample_index, ..., 2])\n",
    "im_2_0 = axes[2][0].imshow(pred[sample_index, ..., 3])\n",
    "# im_2_1 = axes[2][1].imshow(keras.backend.eval(tf.keras.backend.argmax(pred[sample_index], 2)).astype(np.float32))\n",
    "im_2_1 = axes[2][1].imshow(getClassesMap(pred[sample_index]))\n",
    "# im_2_1 = axes[2][1].imshow(pred[sample_index, 20:40, 0:5, 0])\n",
    "im_3_0 = axes[3][0].imshow(y[sample_index, ..., 2])\n",
    "im_3_1 = axes[3][1].imshow(y[sample_index, ..., 1])\n",
    "\n",
    "plt.colorbar(im_0_1, ax=axes[0,1])\n",
    "plt.colorbar(im_1_0, ax=axes[1,0])\n",
    "plt.colorbar(im_1_1, ax=axes[1,1])\n",
    "plt.colorbar(im_2_0, ax=axes[2,0])\n",
    "plt.colorbar(im_2_1, ax=axes[2,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.zeros((pred.shape[1],pred.shape[2], 3)).astype(np.float32)\n",
    "image[:,:,0] = pred[sample_index, ..., 2]\n",
    "image[:,:,1] = pred[sample_index, ..., 1]\n",
    "image[:,:,2] = pred[sample_index, ..., 0]\n",
    "\n",
    "image = (image * 255).astype(np.uint8)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountErrors(y, pred):\n",
    "    ok_cnt = 0\n",
    "    required = {\n",
    "        0: 0,\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "    }\n",
    "    valid = {\n",
    "        0: [0,0,0,0],\n",
    "        1: [0,0,0,0],\n",
    "        2: [0,0,0,0],\n",
    "        3: [0,0,0,0],\n",
    "    }\n",
    "    errors = {\n",
    "        0: [0,0,0,0],\n",
    "        1: [0,0,0,0],\n",
    "        2: [0,0,0,0],\n",
    "        3: [0,0,0,0],\n",
    "    }\n",
    "    \n",
    "    for r in range(y.shape[0]):\n",
    "        for c in range(y.shape[1]):\n",
    "            pred_class_index = np.argmax(pred[r,c,:])\n",
    "            real_class_index = np.argmax(y[r,c,:])\n",
    "            required[real_class_index] += 1\n",
    "            if pred_class_index not in [0,1,2,3] or real_class_index not in [0,1,2,3]:\n",
    "                print(r,c,pred_class_index, real_class_index)\n",
    "                continue\n",
    "#             print(r,c,pred_class_index, real_class_index)\n",
    "            if pred_class_index == real_class_index:\n",
    "                ok_cnt = ok_cnt + 1\n",
    "                valid[real_class_index][pred_class_index] += 1\n",
    "            else:\n",
    "                errors[real_class_index][pred_class_index] += 1\n",
    "    return ok_cnt, r * c, required, valid, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountErrors(y[sample_index], pred[sample_index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 2\n",
    "\n",
    "ok_cnt, cnt, required, valid, errors = CountErrors(y[sample_index], pred[sample_index])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_cnt, required, valid, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 5\n",
    "\n",
    "ok_cnt, cnt, required, valid, errors = CountErrors(y[sample_index], pred[sample_index])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_cnt, cnt, required, valid, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image by image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = './validation/sample_4.png'\n",
    "mask_name = './validation/sample_4_mask.png'\n",
    "x,y = prepare_image_and_mask_for_prediction(img_name, mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model1.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for index in range(4):\n",
    "    img = pred[0, ..., index] / np.max(pred[0, ..., index])\n",
    "    img *= 255.0\n",
    "    img = img.astype(np.uint8)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.imwrite('unet_{}.png'.format(index), img)\n",
    "    \n",
    "img = getClassesMap(pred[0]).astype(np.float32)\n",
    "img = img / np.max(img)\n",
    "img *= 255.0\n",
    "img = img.astype(np.uint8)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "cv2.imwrite('unet_classes.png', img)\n",
    "\n",
    "img = y[0].astype(np.float32)\n",
    "img = y[0, ..., 0] * 0.0001 + y[0, ..., 1] * 1 + y[0, ..., 2] * 2 + y[0, ..., 3] * 3\n",
    "img = img / np.max(img)\n",
    "img *= 255.0\n",
    "img = img.astype(np.uint8)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "cv2.imwrite('unet_gt.png', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(pred[0, ..., 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y[0, ..., 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(getClassesMap(pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(getClassesMap(pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(25, 25))\n",
    "axes[0,0].imshow(x[0])\n",
    "im_0_1 = axes[0][1].imshow(pred[0, ..., 0])\n",
    "\n",
    "plt.colorbar(im_0_1, ax=axes[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model1.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred[:,:,:,2] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_cnt, cnt, required, valid, errors = CountErrors(y[0], pred[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_cnt, cnt, required, valid, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate accuracy for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ok_cnt = 0\n",
    "all_cnt = 0\n",
    "all_required = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 0,\n",
    "}\n",
    "all_valid = {\n",
    "    0: [0,0,0,0],\n",
    "    1: [0,0,0,0],\n",
    "    2: [0,0,0,0],\n",
    "    3: [0,0,0,0],\n",
    "}\n",
    "all_errors = {\n",
    "    0: [0,0,0,0],\n",
    "    1: [0,0,0,0],\n",
    "    2: [0,0,0,0],\n",
    "    3: [0,0,0,0],\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(data.valid.image_paths))):\n",
    "    img_name = data.valid.image_paths[i]\n",
    "    mask_name = data.valid.masks_paths[i]\n",
    "    \n",
    "    x,y = prepare_image_and_mask_for_prediction(img_name, mask_name)\n",
    "    \n",
    "    pred = model1.predict(x)\n",
    "#     pred[:,:,:,2] *= 2\n",
    "    \n",
    "    ok_cnt, cnt, required, valid, errors = CountErrors(y[0], pred[0]) \n",
    "    \n",
    "    all_ok_cnt += ok_cnt\n",
    "    all_cnt += cnt\n",
    "    for j in range(4):\n",
    "        all_required[j] = all_required[j] + required[j]\n",
    "        all_valid[j] = [x + y for x, y in zip(all_valid[j], valid[j])]\n",
    "        all_errors[j] = [x + y for x, y in zip(all_errors[j], errors[j])]\n",
    "\n",
    "image_area = params.GetImageSize() * params.GetImageSize()\n",
    "image_cnt = len(data.valid.image_paths)\n",
    "scale = 1#float(image_area * image_cnt) / 100.\n",
    "\n",
    "all_cnt /= scale\n",
    "all_ok_cnt /= scale\n",
    "for j in range(4):\n",
    "    all_required[j] /= scale\n",
    "    all_valid[j] = [x / scale for x in all_valid[j]]\n",
    "    all_errors[j] = [x / scale for x in all_errors[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_ok_cnt, all_cnt, all_required, all_valid, all_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros([4,4])\n",
    "for x in range(4):\n",
    "    confusion_matrix[x,x] = all_valid[x][x]\n",
    "\n",
    "for real_class in range(4):\n",
    "    for predicted_class in range(4):\n",
    "        if real_class == predicted_class:\n",
    "            continue\n",
    "        confusion_matrix[predicted_class,real_class] = all_errors[real_class][predicted_class]\n",
    "        \n",
    "precision = np.zeros([4])\n",
    "recall = np.zeros([4])\n",
    "for i in range(4):\n",
    "    precision[i] = confusion_matrix[i,i] / sum(confusion_matrix[i,:])\n",
    "    recall[i] = confusion_matrix[i,i] / sum(confusion_matrix[:,i])\n",
    "    \n",
    "def CalcF(precisionV, recallV):\n",
    "    return 2 * precisionV * recallV / (precisionV + recallV)\n",
    "\n",
    "F = [ CalcF(precisionV, recallV) for precisionV, recallV in zip(precision, recall) ]\n",
    "F = np.array(F)\n",
    "\n",
    "confusion_matrix, precision, recall, F\n",
    "np.set_printoptions(precision=2)\n",
    "np.set_printoptions(formatter={\"float_kind\": lambda x: \"%0.2f\" % x})\n",
    "\n",
    "print(\"confusion_matrix = \\n{}\".format(confusion_matrix))\n",
    "print(\"precision = {}\".format(precision))\n",
    "print(\"recall = {}\".format(recall))\n",
    "print(\"F = {}\".format(F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([17674390.00, 427346.00, 107769.00, 14829.00])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
